{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8df084dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "#import causal_convolution_layer\n",
    "#import Dataloader\n",
    "import math\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aea10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(Dataset):\n",
    "    \"\"\"synthetic time series dataset from section 5.1\"\"\"\n",
    "    \n",
    "    def __init__(self,t0=96,N=,transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            t0: previous t0 data points to predict from\n",
    "            N: number of data points\n",
    "            transform: any transformations to be applied to time series\n",
    "        \"\"\"\n",
    "        self.t0 = t0\n",
    "        self.N = N\n",
    "        self.transform = None\n",
    "        \n",
    "        # time points\n",
    "        self.x = torch.cat(N*[torch.arange(0,t0+24).type(torch.float).unsqueeze(0)])\n",
    "\n",
    "        # sinuisoidal signal\n",
    "        # 如果用到自己的数据的话，把下面这块改掉就好\n",
    "        # 注意数据输入格式为（N，Nb of timepoints）\n",
    "        # 其中N为你有多少行ts，以电力系统数据为例，一个客户10天的数据就构成一行ts\n",
    "        # 而nb of timepoints为一行ts中有几个时间点，比如十天小时粒度的，就是 10 * 24 = 240\n",
    "        A1,A2,A3 = 60 * torch.rand(20,N)\n",
    "        A4 = torch.max(A1,A2)        \n",
    "        self.fx = torch.cat([A1.unsqueeze(1)*torch.sin(np.pi*self.x[0,0:12]/6)+72 ,\n",
    "                        A2.unsqueeze(1)*torch.sin(np.pi*self.x[0,12:24]/6)+72 ,\n",
    "                        A3.unsqueeze(1)*torch.sin(np.pi*self.x[0,24:t0]/6)+72,\n",
    "                        A4.unsqueeze(1)*torch.sin(np.pi*self.x[0,t0:t0+24]/12)+72],1)\n",
    "        \n",
    "        # add noise\n",
    "        self.fx = self.fx + torch.randn(self.fx.shape)\n",
    "        \n",
    "        self.masks = self._generate_square_subsequent_mask(t0)\n",
    "                \n",
    "        \n",
    "        # print out shapes to confirm desired output\n",
    "        print(\"x: {}*{}\".format(*list(self.x.shape)),\n",
    "              \"fx: {}*{}\".format(*list(self.fx.shape)))        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.fx)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        \n",
    "        sample = (self.x[idx,:],\n",
    "                  self.fx[idx,:],\n",
    "                  self.masks)\n",
    "        \n",
    "        if self.transform:\n",
    "            sample=self.transform(sample)\n",
    "            \n",
    "        return sample\n",
    "    \n",
    "    def _generate_square_subsequent_mask(self,t0):\n",
    "        mask = torch.zeros(t0+24,t0+24)\n",
    "        for i in range(0,t0):\n",
    "            mask[i,t0:] = 1 \n",
    "        for i in range(t0,t0+24):\n",
    "            mask[i,i+1:] = 1\n",
    "        mask = mask.float().masked_fill(mask == 1, float('-inf'))#.masked_fill(mask == 1, float(0.0))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de71a801",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConv1d(torch.nn.Conv1d):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 stride=1,\n",
    "                 dilation=1,\n",
    "                 groups=1,\n",
    "                 bias=True):\n",
    "\n",
    "        super(CausalConv1d, self).__init__(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=0,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            bias=bias)\n",
    "        \n",
    "        self.__padding = (kernel_size - 1) * dilation\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return super(CausalConv1d, self).forward(F.pad(input, (self.__padding, 0)))\n",
    "\n",
    "\n",
    "class context_embedding(torch.nn.Module):\n",
    "    def __init__(self,in_channels=1,embedding_size=256,k=5):\n",
    "        super(context_embedding,self).__init__()\n",
    "        self.causal_convolution = CausalConv1d(in_channels,embedding_size,kernel_size=k)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.causal_convolution(x)\n",
    "        return F.tanh(x)\n",
    "\n",
    "# model class\n",
    "class TransformerTimeSeries(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Time Series application of transformers based on paper\n",
    "    \n",
    "    causal_convolution_layer parameters:\n",
    "        in_channels: the number of features per time point\n",
    "        out_channels: the number of features outputted per time point\n",
    "        kernel_size: k is the width of the 1-D sliding kernel\n",
    "        \n",
    "    nn.Transformer parameters:\n",
    "        d_model: the size of the embedding vector (input)\n",
    "    \n",
    "    PositionalEncoding parameters:\n",
    "        d_model: the size of the embedding vector (positional vector)\n",
    "        dropout: the dropout to be used on the sum of positional+embedding vector\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(TransformerTimeSeries,self).__init__()\n",
    "        self.input_embedding = causal_convolution_layer.context_embedding(2,256,9)\n",
    "        self.positional_embedding = torch.nn.Embedding(512,256)\n",
    "\n",
    "        \n",
    "        self.decode_layer = torch.nn.TransformerEncoderLayer(d_model=256,nhead=8)\n",
    "        self.transformer_decoder = torch.nn.TransformerEncoder(self.decode_layer, num_layers=3)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(256,1)\n",
    "        \n",
    "    def forward(self,x,y,attention_masks):\n",
    "        \n",
    "        # concatenate observed points and time covariate\n",
    "        # (B*feature_size*n_time_points)\n",
    "        z = torch.cat((y.unsqueeze(1),x.unsqueeze(1)),1)\n",
    "\n",
    "        # input_embedding returns shape (Batch size,embedding size,sequence len) -> need (sequence len,Batch size,embedding_size)\n",
    "        z_embedding = self.input_embedding(z).permute(2,0,1)\n",
    "        \n",
    "        # get my positional embeddings (Batch size, sequence_len, embedding_size) -> need (sequence len,Batch size,embedding_size)\n",
    "        positional_embeddings = self.positional_embedding(x.type(torch.long)).permute(1,0,2)\n",
    "        \n",
    "        input_embedding = z_embedding+positional_embeddings\n",
    "        \n",
    "        transformer_embedding = self.transformer_decoder(input_embedding,attention_masks)\n",
    "\n",
    "        output = self.fc1(transformer_embedding.permute(1,0,2))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33350fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataset, test_dataset, t0, future):\n",
    "    train_dl = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_dl = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "    model = TransformerTimeSeries()\n",
    "\n",
    "    lr = .0005  # learning rate\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    epochs = 50\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    train_epoch_loss = []\n",
    "    eval_epoch_loss = []\n",
    "    Rp_best = 1e5\n",
    "    model_save_path = 'ConvTransformer_nologsparse.pth'\n",
    "    for e, epoch in enumerate(range(epochs)):\n",
    "        train_loss = []\n",
    "        eval_loss = []\n",
    "\n",
    "        l_t = train_epoch(model, train_dl, opt, criterion, t0)\n",
    "        train_loss.append(l_t)\n",
    "\n",
    "        Rp = test_epoch(model, test_dl, t0, future)\n",
    "\n",
    "        if Rp_best > Rp:\n",
    "            Rp_best = Rp\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': opt.state_dict(),\n",
    "                'loss': Rp,\n",
    "            }, model_save_path)\n",
    "\n",
    "        train_epoch_loss.append(np.mean(train_loss))\n",
    "        eval_epoch_loss.append(np.mean(eval_loss))\n",
    "\n",
    "        print(\"Epoch {}: Train loss: {} \\t Validation loss: {} \\t R_p={}\".format(e,\n",
    "                                                                                 np.mean(train_loss),\n",
    "                                                                                 np.mean(eval_loss), Rp))\n",
    "\n",
    "        print(\"Rp best={}\".format(Rp_best))\n",
    "没啥需要特别说明的地方，就是这个作者只是写了一个论文构造数据复现的代码，所以工程性很差，很多地方都写死或者没有抽象出来，拿来即用的同学们需要注意改一下。另外他也没有prediction相关函数，我这边写了一个，仅供参考：\n",
    "\n",
    "prediction\n",
    "def prediction(model, dl, t0, future):\n",
    "    # 预测前先load model， dl就是待预测数据，t0就是前n和时间点，future就是要预测的n个时间点\n",
    "    # 比如你要用一周内前五天的数据训练模型，来预测后两天的值 t0 = 5 * 24 = 120， future = 48\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        observations = []\n",
    "        for step, (x, y, attention_masks) in enumerate(dl):\n",
    "            # x: (batch_size， total_ts_length)\n",
    "            # y: (batch_size, total_ts_length)\n",
    "            # ouput:(batch_size, total_ts_length, 1)\n",
    "            output = model(x, y, attention_masks[0])\n",
    "            history = y[:, :t0].cpu().numpy().tolist()\n",
    "            for p, o in zip(output.squeeze()[:, (t0 - 1):(t0 + future - 1)].cpu().numpy().tolist(),\n",
    "                            y[:, t0:].cpu().numpy().tolist()):  # not missing data\n",
    "\n",
    "                predictions.append(p) # (batch_size, future)\n",
    "                observations.append(o) # (batch_size, future)\n",
    "        num = 0\n",
    "        den = 0\n",
    "        for hist, y_preds, y_trues in zip(history, predictions, observations):\n",
    "            plot_result(hist, y_preds, y_trues, t0)\n",
    "            num_i, den_i = Rp_num_den(y_preds, y_trues, .5)\n",
    "            num += num_i\n",
    "            den += den_i\n",
    "        Rp = (2 * num) / den\n",
    "    return Rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35103bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(history, yhat, ytruth, t0):\n",
    "    # 带上历史值\n",
    "    yhat = history + yhat\n",
    "    ytruth = history + ytruth\n",
    "    # 画图\n",
    "    x = range(len(ytruth))\n",
    "    yhat = np.round(yhat, 2)\n",
    "    ytruth = np.round(ytruth, 2)\n",
    "    plt.figure(facecolor='w')  \n",
    "    plt.plot(range(len(x)), ytruth, 'green', linewidth=1.5, label='ground truth')\n",
    "    plt.plot(range(len(x)), yhat, 'blue', alpha=0.8, linewidth=1.2, label='predict value')\n",
    "    # 画条预测起始线\n",
    "    plt.vlines(t0, yhat.min() * 0.99, yhat.max() * 1.01,\n",
    "               alpha=0.7, colors=\"r\", linestyles=\"dashed\")\n",
    "    # plt.text(0.15, 0.01, error_message, size=10, alpha=0.9, transform=plt.gca().transAxes)  # 相对位置，经验设置值\n",
    "    plt.legend(loc='best')  # 设置标签的位置\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
