{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c9351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ID3决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5be36af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'纹理': {'清晰': {'根蒂': {'蜷缩': 1, '稍蜷': {'色泽': {'浅白': 1, '青绿': 1, '乌黑': {'触感': {'硬滑': 1, '软粘': 0}}}}, '硬挺': 0}}, '稍糊': {'触感': {'软粘': 1, '硬滑': 0}}, '模糊': 0}}\n",
      "分类结果为好瓜\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#计算信息熵\n",
    "def cal_information_entropy(data):\n",
    "    data_label = data.iloc[:,-1]\n",
    "    label_class =data_label.value_counts() #总共有多少类\n",
    "    Ent = 0\n",
    "    for k in label_class.keys():\n",
    "        p_k = label_class[k]/len(data_label)\n",
    "        Ent += -p_k*np.log2(p_k)\n",
    "    return Ent\n",
    "\n",
    "#计算给定数据属性a的信息增益\n",
    "def cal_information_gain(data, a):\n",
    "    Ent = cal_information_entropy(data)\n",
    "    feature_class = data[a].value_counts() #特征有多少种可能\n",
    "    gain = 0\n",
    "    for v in feature_class.keys():\n",
    "        weight = feature_class[v]/data.shape[0]\n",
    "        Ent_v = cal_information_entropy(data.loc[data[a] == v])\n",
    "        gain += weight*Ent_v\n",
    "    return Ent - gain\n",
    "\n",
    "#获取标签最多的那一类\n",
    "def get_most_label(data):\n",
    "    data_label = data.iloc[:,-1]\n",
    "    label_sort = data_label.value_counts(sort=True)\n",
    "    return label_sort.keys()[0]\n",
    "\n",
    "#挑选最优特征，即信息增益最大的特征\n",
    "def get_best_feature(data):\n",
    "    features = data.columns[:-1]\n",
    "    res = {}\n",
    "    for a in features:\n",
    "        temp = cal_information_gain(data, a)\n",
    "        res[a] = temp\n",
    "    res = sorted(res.items(),key=lambda x:x[1],reverse=True)\n",
    "    return res[0][0]\n",
    "\n",
    "##将数据转化为（属性值：数据）的元组形式返回，并删除之前的特征列\n",
    "def drop_exist_feature(data, best_feature):\n",
    "    attr = pd.unique(data[best_feature])\n",
    "    new_data = [(nd, data[data[best_feature] == nd]) for nd in attr]\n",
    "    new_data = [(n[0], n[1].drop([best_feature], axis=1)) for n in new_data]\n",
    "    return new_data\n",
    "\n",
    "#创建决策树\n",
    "def create_tree(data):\n",
    "    data_label = data.iloc[:,-1]\n",
    "    if len(data_label.value_counts()) == 1: #只有一类\n",
    "        return data_label.values[0]\n",
    "    if all(len(data[i].value_counts()) == 1 for i in data.iloc[:,:-1].columns): #所有数据的特征值一样，选样本最多的类作为分类结果\n",
    "        return get_most_label(data)\n",
    "    best_feature = get_best_feature(data) #根据信息增益得到的最优划分特征\n",
    "    Tree = {best_feature:{}} #用字典形式存储决策树\n",
    "    exist_vals = pd.unique(data[best_feature]) #当前数据下最佳特征的取值\n",
    "    if len(exist_vals) != len(column_count[best_feature]): #如果特征的取值相比于原来的少了\n",
    "        no_exist_attr = set(column_count[best_feature]) - set(exist_vals) #少的那些特征\n",
    "        for no_feat in no_exist_attr:\n",
    "            Tree[best_feature][no_feat] = get_most_label(data) #缺失的特征分类为当前类别最多的\n",
    "\n",
    "    for item in drop_exist_feature(data,best_feature): #根据特征值的不同递归创建决策树\n",
    "        Tree[best_feature][item[0]] = create_tree(item[1])\n",
    "    return Tree\n",
    "\n",
    "#{'纹理': {'清晰': {'根蒂': {'蜷缩': 1, '稍蜷': {'色泽': {'青绿': 1, '乌黑': {'触感': {'硬滑': 1, '软粘': 0}}}}, '硬挺': 0}}, '稍糊': {'触感': {'软粘': 1, '硬滑': 0}}, '模糊': 0}}\n",
    "def predict(Tree , test_data):\n",
    "    first_feature = list(Tree.keys())[0]\n",
    "    second_dict = Tree[first_feature]\n",
    "    input_first = test_data.get(first_feature)\n",
    "    input_value = second_dict[input_first]\n",
    "    if isinstance(input_value , dict): #判断分支还是不是字典\n",
    "        class_label = predict(input_value, test_data)\n",
    "    else:\n",
    "        class_label = input_value\n",
    "    return class_label\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #读取数据\n",
    "    import pandas as pd\n",
    "    data = pd.read_csv('西瓜数据集2.0.txt')\n",
    "    del data['编号']\n",
    "    for i in  range(data.shape[0]):\n",
    "        if data['好瓜'][i]=='是':\n",
    "            data['好瓜'][i]=1\n",
    "        else:\n",
    "            data['好瓜'][i]=0\n",
    "\n",
    "    #统计每个特征的取值情况作为全局变量\n",
    "    column_count = dict([(ds, list(pd.unique(data[ds]))) for ds in data.iloc[:, :-1].columns])\n",
    "\n",
    "    #创建决策树\n",
    "    dicision_Tree = create_tree(data)\n",
    "    print(dicision_Tree)\n",
    "    #测试数据\n",
    "    test_data_1 = {'色泽':'青绿','根蒂':'蜷缩','敲声':'浊响','纹理':'稍糊','脐部':'凹陷','触感':'硬滑'}\n",
    "    test_data_2 = {'色泽': '乌黑', '根蒂': '稍蜷', '敲声': '浊响', '纹理': '清晰', '脐部': '凹陷', '触感': '硬滑'}\n",
    "    result = predict(dicision_Tree,test_data_2)\n",
    "    print('分类结果为'+'好瓜'if result == 1 else '坏瓜')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cb34f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#C4.5决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "906d3c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'纹理': {'清晰': {'触感': {'硬滑': '是', '软粘': {'编号': {1: '否', 2: '否', 3: '否', 4: '否', 5: '否', 7: '否', 8: '否', 9: '否', 11: '否', 12: '否', 13: '否', 14: '否', 16: '否', 17: '否', 6: '是', 10: '否', 15: '否'}}}}, '稍糊': {'触感': {'软粘': '是', '硬滑': '否'}}, '模糊': '否'}}\n",
      "坏瓜\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#计算信息熵\n",
    "def cal_information_entropy(data):\n",
    "    data_label = data.iloc[:,-1]\n",
    "    label_class =data_label.value_counts() #总共有多少类\n",
    "    Ent = 0\n",
    "    for k in label_class.keys():\n",
    "        p_k = label_class[k]/len(data_label)\n",
    "        Ent += -p_k*np.log2(p_k)\n",
    "    return Ent\n",
    "\n",
    "#计算给定数据属性a的信息增益\n",
    "def cal_information_gain(data, a):\n",
    "    Ent = cal_information_entropy(data)\n",
    "    feature_class = data[a].value_counts() #特征有多少种可能\n",
    "    gain = 0\n",
    "    for v in feature_class.keys():\n",
    "        weight = feature_class[v]/data.shape[0]\n",
    "        Ent_v = cal_information_entropy(data.loc[data[a] == v])\n",
    "        gain += weight*Ent_v\n",
    "    return Ent - gain\n",
    "\n",
    "def cal_gain_ratio(data , a):\n",
    "    #先计算固有值intrinsic_value\n",
    "    IV_a = 0\n",
    "    feature_class = data[a].value_counts()  # 特征有多少种可能\n",
    "    for v in feature_class.keys():\n",
    "        weight = feature_class[v]/data.shape[0]\n",
    "        IV_a += -weight*np.log2(weight)\n",
    "    gain_ration = cal_information_gain(data,a)/IV_a\n",
    "    return gain_ration\n",
    "\n",
    "#获取标签最多的那一类\n",
    "def get_most_label(data):\n",
    "    data_label = data.iloc[:,-1]\n",
    "    label_sort = data_label.value_counts(sort=True)\n",
    "    return label_sort.keys()[0]\n",
    "\n",
    "#挑选最优特征，即在信息增益大于平均水平的特征中选取增益率最高的特征\n",
    "def get_best_feature(data):\n",
    "    features = data.columns[:-1]\n",
    "    res = {}\n",
    "    for a in features:\n",
    "        temp = cal_information_gain(data, a)\n",
    "        gain_ration = cal_gain_ratio(data,a)\n",
    "        res[a] = (temp,gain_ration)\n",
    "    res = sorted(res.items(),key=lambda x:x[1][0],reverse=True) #按信息增益排名\n",
    "    res_avg = sum([x[1][0] for x in res])/len(res) #信息增益平均水平\n",
    "    good_res = [x for x in res if x[1][0] >= res_avg] #选取信息增益高于平均水平的特征\n",
    "    result =sorted(good_res,key=lambda x:x[1][1],reverse=True) #将信息增益高的特征按照增益率进行排名\n",
    "    return result[0][0] #返回高信息增益中增益率最大的特征\n",
    "\n",
    "##将数据转化为（属性值：数据）的元组形式返回，并删除之前的特征列\n",
    "def drop_exist_feature(data, best_feature):\n",
    "    attr = pd.unique(data[best_feature])\n",
    "    new_data = [(nd, data[data[best_feature] == nd]) for nd in attr]\n",
    "    new_data = [(n[0], n[1].drop([best_feature], axis=1)) for n in new_data]\n",
    "    return new_data\n",
    "\n",
    "#创建决策树\n",
    "def create_tree(data):\n",
    "    data_label = data.iloc[:,-1]\n",
    "    if len(data_label.value_counts()) == 1: #只有一类\n",
    "        return data_label.values[0]\n",
    "    if all(len(data[i].value_counts()) == 1 for i in data.iloc[:,:-1].columns): #所有数据的特征值一样，选样本最多的类作为分类结果\n",
    "        return get_most_label(data)\n",
    "    best_feature = get_best_feature(data) #根据信息增益得到的最优划分特征\n",
    "    Tree = {best_feature:{}} #用字典形式存储决策树\n",
    "    exist_vals = pd.unique(data[best_feature])  # 当前数据下最佳特征的取值\n",
    "    if len(exist_vals) != len(column_count[best_feature]):  # 如果特征的取值相比于原来的少了\n",
    "        no_exist_attr = set(column_count[best_feature]) - set(exist_vals)  # 少的那些特征\n",
    "        for no_feat in no_exist_attr:\n",
    "            Tree[best_feature][no_feat] = get_most_label(data)  # 缺失的特征分类为当前类别最多的\n",
    "    for item in drop_exist_feature(data,best_feature): #根据特征值的不同递归创建决策树\n",
    "        Tree[best_feature][item[0]] = create_tree(item[1])\n",
    "    return Tree\n",
    "\n",
    "def predict(Tree , test_data):\n",
    "    first_feature = list(Tree.keys())[0]\n",
    "    second_dict = Tree[first_feature]\n",
    "    input_first = test_data.get(first_feature)\n",
    "    input_value = second_dict[input_first]\n",
    "    if isinstance(input_value , dict): #判断分支还是不是字典\n",
    "        class_label = predict(input_value, test_data)\n",
    "    else:\n",
    "        class_label = input_value\n",
    "    return class_label\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #读取数据\n",
    "    data = pd.read_csv('西瓜数据集2.0.txt')\n",
    "    # 统计每个特征的取值情况作为全局变量\n",
    "    column_count = dict([(ds, list(pd.unique(data[ds]))) for ds in data.iloc[:, :-1].columns])\n",
    "\n",
    "    #创建决策树\n",
    "    dicision_Tree = create_tree(data)\n",
    "    print(dicision_Tree)\n",
    "    #测试数据\n",
    "    test_data_1 = {'色泽':'青绿','根蒂':'蜷缩','敲声':'浊响','纹理':'稍糊','脐部':'凹陷','触感':'硬滑'}\n",
    "    test_data_2 = {'色泽': '乌黑', '根蒂': '稍蜷', '敲声': '浊响', '纹理': '清晰', '脐部': '凹陷', '触感': '硬滑'}\n",
    "    result = predict(dicision_Tree,test_data_2)\n",
    "    print('分类结果为'+'好瓜'if result == 1 else '坏瓜')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fb8014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CART决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c778133b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'编号': {1: '是', 'Others': {'根蒂': {'硬挺': '否', 'Others': {'纹理': {'模糊': '否', 'Others': {'色泽': {'浅白': {'敲声': {'浊响': '是', 'Others': '否'}}, 'Others': {'触感': {'软粘': '是', 'Others': {'敲声': {'浊响': {'脐部': {'稍凹': '是', 'Others': '是'}}, 'Others': {'脐部': {'凹陷': '是', 'Others': '否'}}}}}}}}}}}}}}\n",
      "坏瓜\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#计算基尼指数\n",
    "def gini(data):\n",
    "    data_label = data.iloc[:, -1]\n",
    "    label_num = data_label.value_counts() #有几类，每一类的数量\n",
    "    res = 0\n",
    "    for k in label_num.keys():\n",
    "        p_k = label_num[k]/len(data_label)\n",
    "        res += p_k ** 2\n",
    "    return 1 - res\n",
    "\n",
    "# 计算每个特征取值的基尼指数，找出最优切分点\n",
    "def gini_index(data,a):\n",
    "    feature_class = data[a].value_counts()\n",
    "    res = []\n",
    "    for feature in feature_class.keys():\n",
    "        weight = feature_class[feature]/len(data)\n",
    "        gini_value = gini(data.loc[data[a] == feature])\n",
    "        res.append([feature, weight * gini_value])\n",
    "    res = sorted(res, key = lambda x: x[-1])\n",
    "    return res[0]\n",
    "\n",
    "#获取标签最多的那一类\n",
    "def get_most_label(data):\n",
    "    data_label = data.iloc[:,-1]\n",
    "    label_sort = data_label.value_counts(sort=True)\n",
    "    return label_sort.keys()[0]\n",
    "\n",
    "#挑选最优特征，即基尼指数最小的特征\n",
    "def get_best_feature(data):\n",
    "    features = data.columns[:-1]\n",
    "    res = {}\n",
    "    for a in features:\n",
    "        temp = gini_index(data, a) #temp是列表，【feature_value, gini】\n",
    "        res[a] = temp\n",
    "    res = sorted(res.items(),key=lambda x:x[1][1])\n",
    "    return res[0][0], res[0][1][0]\n",
    "\n",
    "def drop_exist_feature(data, best_feature, value, type):\n",
    "    attr = pd.unique(data[best_feature]) #表示特征所有取值的数组\n",
    "    if type == 1: #使用特征==value的值进行划分\n",
    "        new_data = [[value], data.loc[data[best_feature] == value]]\n",
    "    else:\n",
    "        new_data = [attr, data.loc[data[best_feature] != value]]\n",
    "    new_data[1] = new_data[1].drop([best_feature], axis=1) #删除该特征\n",
    "    return new_data\n",
    "\n",
    "#创建决策树\n",
    "def create_tree(data):\n",
    "    data_label = data.iloc[:,-1]\n",
    "    if len(data_label.value_counts()) == 1: #只有一类\n",
    "        return data_label.values[0]\n",
    "    if all(len(data[i].value_counts()) == 1 for i in data.iloc[:,:-1].columns): #所有数据的特征值一样，选样本最多的类作为分类结果\n",
    "        return get_most_label(data)\n",
    "    best_feature, best_feature_value = get_best_feature(data) #根据信息增益得到的最优划分特征\n",
    "    Tree = {best_feature:{}} #用字典形式存储决策树\n",
    "\n",
    "    Tree[best_feature][best_feature_value] = create_tree(drop_exist_feature(data, best_feature, best_feature_value, 1)[1])\n",
    "    Tree[best_feature]['Others'] = create_tree(drop_exist_feature(data, best_feature, best_feature_value, 2)[1])\n",
    "    return Tree\n",
    "\n",
    "def predict(Tree , test_data):\n",
    "    first_feature = list(Tree.keys())[0] #第一个特征\n",
    "    second_dict = Tree[first_feature] #第一个特征后面的字典\n",
    "    input_first = test_data.get(first_feature) #预测输入的第一个特征值是多少\n",
    "    input_value = second_dict[input_first] if input_first == list(second_dict.keys())[0] else second_dict['Others'] #预测输入对应的字典\n",
    "    if isinstance(input_value , dict): #判断分支还是不是字典\n",
    "        class_label = predict(input_value, test_data)\n",
    "    else:\n",
    "        class_label = input_value\n",
    "    return class_label\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #读取数据\n",
    "    data = pd.read_csv('西瓜数据集2.0.txt')\n",
    "\n",
    "    #创建决策树\n",
    "    dicision_Tree = create_tree(data)\n",
    "    print(dicision_Tree)\n",
    "    #测试数据\n",
    "    test_data_1 = {'色泽':'青绿','根蒂':'蜷缩','敲声':'浊响','纹理':'稍糊','脐部':'凹陷','触感':'硬滑'}\n",
    "    test_data_2 = {'色泽': '乌黑', '根蒂': '稍蜷', '敲声': '浊响', '纹理': '清晰', '脐部': '凹陷', '触感': '硬滑'}\n",
    "    result = predict(dicision_Tree,test_data_2)\n",
    "    print('分类结果为'+'好瓜'if result == 1 else '坏瓜')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6de220",
   "metadata": {},
   "outputs": [],
   "source": [
    "#决策树剪枝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68830fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "剪枝前的决策树:\n",
      "{'脐部': {'prun_label': 1, '凹陷': {'色泽': {'prun_label': 1, '青绿': 1, '乌黑': 1, '浅白': 0}}, '稍凹': {'根蒂': {'prun_label': 1, '稍蜷': {'色泽': {'prun_label': 1, '青绿': 1, '乌黑': {'纹理': {'prun_label': 1, '稍糊': 1, '清晰': 0, '模糊': 1}}, '浅白': 1}}, '蜷缩': 0, '硬挺': 1}}, '平坦': 0}}\n",
      "剪枝前的测试集准确率: 0.42857142857142855\n",
      "--------------------剪枝--------------------\n",
      "剪枝后的决策树:\n",
      "{'脐部': {'凹陷': 1, '稍凹': {'根蒂': {'稍蜷': {'色泽': {'青绿': 1, '乌黑': 1, '浅白': 1}}, '蜷缩': 0, '硬挺': 1}}, '平坦': 0}}\n",
      "剪枝后的测试集准确率: 0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "##将数据转化为（属性值：数据）的元组形式返回，并删除之前的特征列\n",
    "def drop_exist_feature(data, best_feature):\n",
    "    attr = pd.unique(data[best_feature])\n",
    "    new_data = [(nd, data[data[best_feature] == nd]) for nd in attr]\n",
    "    new_data = [(n[0], n[1].drop([best_feature], axis=1)) for n in new_data]\n",
    "    return new_data\n",
    "\n",
    "# 预测单条数据\n",
    "def predict(Tree , test_data):\n",
    "    first_feature = list(Tree.keys())[0]\n",
    "    second_dict = Tree[first_feature]\n",
    "    input_first = test_data.get(first_feature)\n",
    "    input_value = second_dict[input_first]\n",
    "    if isinstance(input_value , dict): #判断分支还是不是字典\n",
    "        class_label = predict(input_value, test_data)\n",
    "    else:\n",
    "        class_label = input_value\n",
    "    return class_label\n",
    "\n",
    "#测试很多案例，话返回准确率\n",
    "def predict_more(Tree, test_data, test_label):\n",
    "    cnt = 0\n",
    "    #计算如果该节点不剪枝的准确率\n",
    "    for i in range(len(test_data)):\n",
    "        after_data = test_data.reset_index().loc[i].to_dict()\n",
    "        pred = predict(Tree,  after_data)\n",
    "        if pred == test_label[i]:\n",
    "            cnt += 1\n",
    "    return cnt / len(test_label)\n",
    "\n",
    "#用于预测节点剪枝后的预测正确数\n",
    "def equalNums(label, featPreLabel):\n",
    "    res = 0\n",
    "    for l in label:\n",
    "        if l == featPreLabel:\n",
    "            res += 1\n",
    "    return res\n",
    "\n",
    "# 后剪枝\n",
    "def post_prunning(tree , test_data , test_label , names):\n",
    "    newTree = tree.copy() #copy是浅拷贝\n",
    "    names = np.asarray(names)\n",
    "    # 取决策节点的名称 即特征的名称\n",
    "    featName = list(tree.keys())[0]\n",
    "    # 取特征的列\n",
    "    featCol = np.argwhere(names == featName)[0][0]\n",
    "    names = np.delete(names, [featCol]) #删掉使用过的特征\n",
    "    newTree[featName] = tree[featName].copy() #取值\n",
    "    featValueDict = newTree[featName] #当前特征下面的取值情况\n",
    "    featPreLabel = featValueDict.pop(\"prun_label\") #如果当前节点剪枝的话是什么标签，并删除_vpdl\n",
    "\n",
    "    # 分割测试数据 如果有数据 则进行测试或递归调用:\n",
    "    split_data = drop_exist_feature(test_data,featName) #删除该特征，按照该特征的取值重新划分数据\n",
    "    split_data = dict(split_data)\n",
    "\n",
    "    for featValue in featValueDict.keys(): #每个特征的值\n",
    "        if type(featValueDict[featValue]) == dict: #如果下一层还是字典，说明还是子树\n",
    "\n",
    "            split_data_feature = split_data[featValue] #特征某个取值的数据，如“脐部”特征值为“凹陷”的数据\n",
    "            split_data_lable = split_data[featValue].iloc[:, -1].values\n",
    "            # 递归到下一个节点\n",
    "            newTree[featName][featValue] = post_prunning(featValueDict[featValue],split_data_feature,split_data_lable,split_data_feature.columns)\n",
    "\n",
    "    # 根据准确率判断是否剪枝，注意这里的准确率是到达该节点数据预测正确的准确率，而不是整体数据集的准确率\n",
    "    # 因为在修改当前节点时，走到其他节点的数据的预测结果是不变的，所以只需要计算走到当前节点的数据预测对了没有即可\n",
    "    ratioPreDivision = equalNums(test_label, featPreLabel) / test_label.size #判断测试集的数据如果剪枝的准确率\n",
    "\n",
    "    #计算如果该节点不剪枝的准确率\n",
    "    ratioAfterDivision = predict_more(newTree, test_data, test_label)\n",
    "\n",
    "    if ratioAfterDivision < ratioPreDivision:\n",
    "        newTree = featPreLabel # 返回剪枝结果，其实也就是走到当前节点的数据最多的那一类\n",
    "\n",
    "    return newTree\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #读取数据\n",
    "    train_data = pd.read_csv('train_data.csv')\n",
    "    test_data = pd.read_csv('test_data.csv')\n",
    "    test_data_label = test_data.iloc[:, -1].values\n",
    "    names = test_data.columns\n",
    "\n",
    "    dicision_Tree = {\"脐部\": {\"prun_label\": 1\n",
    "                                   , '凹陷': {'色泽':{\"prun_label\": 1, '青绿': 1, '乌黑': 1, '浅白': 0}}\n",
    "                                   , '稍凹': {'根蒂':{\"prun_label\": 1\n",
    "                                                  , '稍蜷': {'色泽': {\"prun_label\": 1\n",
    "                                                                  , '青绿': 1\n",
    "                                                                  , '乌黑': {'纹理': {\"prun_label\": 1\n",
    "                                                                               , '稍糊': 1, '清晰': 0, '模糊': 1}}\n",
    "                                                                  , '浅白': 1}}\n",
    "                                                  , '蜷缩': 0\n",
    "                                                  , '硬挺': 1}}\n",
    "                                   , '平坦': 0}}\n",
    "    print('剪枝前的决策树:')\n",
    "    print(dicision_Tree)\n",
    "    print('剪枝前的测试集准确率: {}'.format(predict_more(dicision_Tree, test_data, test_data_label)))\n",
    "\n",
    "    print('-'*20  + '剪枝' + '-'*20)\n",
    "    new_tree = post_prunning(dicision_Tree,test_data , test_data_label , names)\n",
    "    print('剪枝后的决策树:')\n",
    "    print(new_tree)\n",
    "    print('剪枝后的测试集准确率: {}'.format(predict_more(new_tree, test_data, test_data_label)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2818763",
   "metadata": {},
   "outputs": [],
   "source": [
    "#连续值决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d323053f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'纹理': {'清晰': {'密度<0.3815': {'是': 0, '否': 1}}, '稍糊': {'触感': {'软粘': 1, '硬滑': 0}}, '模糊': 0}}\n",
      "预测结果为:好瓜\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#计算信息熵\n",
    "def cal_information_entropy(data):\n",
    "    data_label = data.iloc[:,-1]\n",
    "    label_class =data_label.value_counts() #总共有多少类\n",
    "    Ent = 0\n",
    "    for k in label_class.keys():\n",
    "        p_k = label_class[k]/len(data_label)\n",
    "        Ent += -p_k*np.log2(p_k)\n",
    "    return Ent\n",
    "\n",
    "#对于离散特征a，计算给定数据属性a的信息增益\n",
    "def cal_information_gain(data, a):\n",
    "    Ent = cal_information_entropy(data)\n",
    "    feature_class = data[a].value_counts() #特征有多少种可能\n",
    "    gain = 0\n",
    "    for v in feature_class.keys():\n",
    "        weight = feature_class[v]/data.shape[0]\n",
    "        Ent_v = cal_information_entropy(data.loc[data[a] == v])\n",
    "        gain += weight*Ent_v\n",
    "    return Ent - gain\n",
    "\n",
    "#对于连续特征b，计算给定数据属性b的信息增益\n",
    "def cal_information_gain_continuous(data, a):\n",
    "    n = len(data) #总共有n条数据，会产生n-1个划分点，选择信息增益最大的作为最优划分点\n",
    "    data_a_value = sorted(data[a].values) #从小到大排序\n",
    "    Ent = cal_information_entropy(data) #原始数据集的信息熵Ent(D)\n",
    "    select_points = []\n",
    "    for i in range(n-1):\n",
    "        val = (data_a_value[i] + data_a_value[i+1]) / 2 #两个值中间取值为划分点\n",
    "        data_left = data.loc[data[a]<val]\n",
    "        data_right = data.loc[data[a]>val]\n",
    "        ent_left = cal_information_entropy(data_left)\n",
    "        ent_right = cal_information_entropy(data_right)\n",
    "        result = Ent - len(data_left)/n * ent_left - len(data_right)/n * ent_right\n",
    "        select_points.append([val, result])\n",
    "    select_points.sort(key = lambda x : x[1], reverse= True) #按照信息增益排序\n",
    "    return select_points[0][0], select_points[0][1] #返回信息增益最大的点, 以及对应的信息增益\n",
    "\n",
    "#获取标签最多的那一类\n",
    "def get_most_label(data):\n",
    "    data_label = data.iloc[:,-1]\n",
    "    label_sort = data_label.value_counts(sort=True)\n",
    "    return label_sort.keys()[0]\n",
    "\n",
    "#获取最佳划分特征\n",
    "def get_best_feature(data):\n",
    "    features = data.columns[:-1]\n",
    "    res = {}\n",
    "    for a in features:\n",
    "        if a in continuous_features:\n",
    "            temp_val, temp = cal_information_gain_continuous(data, a)\n",
    "            res[a] = [temp_val, temp]\n",
    "        else:\n",
    "            temp = cal_information_gain(data, a)\n",
    "            res[a] = [-1, temp] #离散值没有划分点，用-1代替\n",
    "\n",
    "    res = sorted(res.items(),key=lambda x:x[1][1],reverse=True)\n",
    "    return res[0][0],res[0][1][0]\n",
    "\n",
    "#将数据转化为（属性值：数据）的元组形式返回，并删除之前的特征列，只针对离散数据\n",
    "def drop_exist_feature(data, best_feature):\n",
    "    attr = pd.unique(data[best_feature])\n",
    "    new_data = [(nd, data[data[best_feature] == nd]) for nd in attr]\n",
    "    new_data = [(n[0], n[1].drop([best_feature], axis=1)) for n in new_data]\n",
    "    return new_data\n",
    "\n",
    "#创建决策树\n",
    "def create_tree(data):\n",
    "    data_label = data.iloc[:,-1]\n",
    "    if len(data_label.value_counts()) == 1: #只有一类\n",
    "        return data_label.values[0]\n",
    "    if all(len(data[i].value_counts()) == 1 for i in data.iloc[:,:-1].columns): #所有数据的特征值一样，选样本最多的类作为分类结果\n",
    "        return get_most_label(data)\n",
    "    best_feature, best_feature_val = get_best_feature(data) #根据信息增益得到的最优划分特征\n",
    "    if best_feature in continuous_features: #连续值\n",
    "        node_name = best_feature + '<' + str(best_feature_val)\n",
    "        Tree = {node_name:{}} #用字典形式存储决策树\n",
    "        Tree[node_name]['是'] = create_tree(data.loc[data[best_feature] < best_feature_val])\n",
    "        Tree[node_name]['否'] = create_tree(data.loc[data[best_feature] > best_feature_val])\n",
    "    else:\n",
    "        Tree = {best_feature:{}}\n",
    "        exist_vals = pd.unique(data[best_feature])  # 当前数据下最佳特征的取值\n",
    "        if len(exist_vals) != len(column_count[best_feature]):  # 如果特征的取值相比于原来的少了\n",
    "            no_exist_attr = set(column_count[best_feature]) - set(exist_vals)  # 少的那些特征\n",
    "            for no_feat in no_exist_attr:\n",
    "                Tree[best_feature][no_feat] = get_most_label(data)  # 缺失的特征分类为当前类别最多的\n",
    "        for item in drop_exist_feature(data, best_feature):  # 根据特征值的不同递归创建决策树\n",
    "            Tree[best_feature][item[0]] = create_tree(item[1])\n",
    "    return Tree\n",
    "\n",
    "#根据创建的决策树进行分类\n",
    "def predict(Tree , test_data):\n",
    "    first_feature = list(Tree.keys())[0]\n",
    "    if (feature_name:= first_feature.split('<')[0]) in continuous_features:\n",
    "        second_dict = Tree[first_feature]\n",
    "        val = float(first_feature.split('<')[-1])\n",
    "        input_first = test_data.get(feature_name)\n",
    "        if input_first < val:\n",
    "            input_value = second_dict['是']\n",
    "        else:\n",
    "            input_value = second_dict['否']\n",
    "    else:\n",
    "        second_dict = Tree[first_feature]\n",
    "        input_first = test_data.get(first_feature)\n",
    "        input_value = second_dict[input_first]\n",
    "    if isinstance(input_value , dict): #判断分支还是不是字典\n",
    "        class_label = predict(input_value, test_data)\n",
    "    else:\n",
    "        class_label = input_value\n",
    "    return class_label\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = pd.read_csv('西瓜数据集3.0 (1).csv')\n",
    "    # 统计每个特征的取值情况作为全局变量\n",
    "    column_count = dict([(ds, list(pd.unique(data[ds]))) for ds in data.iloc[:, :-1].columns])\n",
    "\n",
    "    test = cal_information_gain_continuous(data, '密度')\n",
    "    continuous_features = ['密度', '含糖率']  #先标注连续值\n",
    "    dicision_tree = create_tree(data)\n",
    "    print(dicision_tree)\n",
    "    test_data =  {'色泽':'青绿','根蒂':'蜷缩','敲声':'浊响','纹理':'清晰','脐部':'凹陷','触感':'硬滑','密度':0.51,'含糖率':0.3}\n",
    "    result = predict(dicision_tree, test_data)\n",
    "    print('预测结果为:{}'.format('好瓜' if result == 1 else '坏瓜'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b803daa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#缺失值决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6bfc617a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'编号': {1: '是', 2: '是', 3: '是', 4: '是', 5: '是', 6: '是', 7: '是', 8: '是', 9: '否', 10: '否', 11: '否', 12: '否', 13: '否', 14: '否', 15: '否', 16: '否', 17: '否'}}\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#计算信息熵\n",
    "def cal_information_entropy(data):\n",
    "    data_label = data.iloc[:,-1] #类别标签\n",
    "    label_class = data_label.value_counts() #总共有多少类\n",
    "    Ent = 0\n",
    "    D_W_x = sum(data['weights']) #整体数据的权值和\n",
    "    for k in label_class.keys():\n",
    "        D_k_wx = sum(data.loc[data_label == k]['weights']) #每个类别的权值和\n",
    "        p_k = D_k_wx / D_W_x\n",
    "        Ent += -p_k*np.log2(p_k)\n",
    "    return Ent\n",
    "\n",
    "#计算信息增益\n",
    "#计算给定数据属性a的信息增益\n",
    "def cal_information_gain(data, a, p): #p表示课本中的ρ，即非空数据占整体数据样本的比例\n",
    "    Ent = cal_information_entropy(data) #整体信息熵\n",
    "    feature_class = data[a].value_counts() #特征有多少种可能\n",
    "    gain = 0\n",
    "    for v in feature_class.keys():\n",
    "        r = sum(data[data[a] == v]['weights'])/sum(data['weights']) # 课本上的r，表示该特征某一个取值的样本权值和占整体样本权值和的比值\n",
    "        Ent_v = cal_information_entropy(data.loc[data[a] == v])\n",
    "        gain += r*Ent_v\n",
    "    return p*(Ent - gain)\n",
    "\n",
    "#获取标签最多的那一类\n",
    "def get_most_label(data):\n",
    "    data_label = data.iloc[:,-1]\n",
    "    label_sort = data_label.value_counts(sort=True)\n",
    "    return label_sort.keys()[0]\n",
    "\n",
    "#挑选最优特征，即信息增益最大的特征\n",
    "def get_best_feature(data):\n",
    "    features = data.columns[1:-1]\n",
    "    res = {}\n",
    "    for a in features:\n",
    "        data_not_null = data.dropna(axis=0,subset = [a]) #该特征不为空的数据\n",
    "        p = sum(data_not_null['weights']) / sum(data['weights']) #占比\n",
    "        temp = cal_information_gain(data_not_null, a, p ) #用非空的数据去算信息增益,最后乘上p\n",
    "        res[a] = temp\n",
    "    res = sorted(res.items(),key=lambda x:x[1],reverse=True) #按照信息增益排名\n",
    "    return res[0][0]\n",
    "\n",
    "##将数据转化为（属性值：数据）的元组形式返回，并删除之前的特征列\n",
    "def drop_exist_feature(data, best_feature):\n",
    "    attr = pd.unique(data.dropna(axis=0,subset = [best_feature])[best_feature]) #最佳划分特征的取值可能,先不包括空值\n",
    "    res = []\n",
    "    data_non = data[data[best_feature].isna()] #该特征为空的数据\n",
    "    for val in attr:\n",
    "        new_data = data[data[best_feature] == val]\n",
    "        p = len(new_data) / len(data) #计算当前取值占比\n",
    "        if len(data_non) > 0: #如果有的话\n",
    "            data_non_cp = data_non.copy()\n",
    "            data_non_cp['weights'] *= p #权值变小\n",
    "            new_data = new_data.append(data_non_cp) #并入数据\n",
    "        res.append((val, new_data))\n",
    "    final_data = [(n[0], n[1].drop([best_feature], axis=1)) for n in res] #删除用过的特征\n",
    "    return final_data\n",
    "\n",
    "#创建决策树\n",
    "def create_tree(data):\n",
    "    data_label = data.iloc[:,-1]\n",
    "    if len(data_label.value_counts()) == 1: #只有一类\n",
    "        return data_label.values[0]\n",
    "    if all(len(data[i].value_counts()) == 1 for i in data.iloc[:,:-1].columns): #所有数据的特征值一样，选样本最多的类作为分类结果\n",
    "        return get_most_label(data)\n",
    "    best_feature = get_best_feature(data) #根据信息增益得到的最优划分特征\n",
    "    Tree = {best_feature:{}} #用字典形式存储决策树\n",
    "    exist_vals = pd.unique(data[best_feature])  # 当前数据下最佳特征的取值\n",
    "    if len(exist_vals) != len(column_count[best_feature]):  # 如果特征的取值相比于原来的少了\n",
    "        no_exist_attr = set(column_count[best_feature]) - set(exist_vals)  # 少的那些特征\n",
    "        for no_feat in no_exist_attr:\n",
    "            Tree[best_feature][no_feat] = get_most_label(data)  # 缺失的特征分类为当前类别最多的\n",
    "    for item in drop_exist_feature(data,best_feature): #根据特征值的不同递归创建决策树\n",
    "        Tree[best_feature][item[0]] = create_tree(item[1])\n",
    "    return Tree\n",
    "\n",
    "def predict(Tree , test_data):\n",
    "    first_feature = list(Tree.keys())[0]\n",
    "    second_dict = Tree[first_feature]\n",
    "    input_first = test_data.get(first_feature)\n",
    "    input_value = second_dict[input_first]\n",
    "    if isinstance(input_value , dict): #判断分支还是不是字典\n",
    "        class_label = predict(input_value, test_data)\n",
    "    else:\n",
    "        class_label = input_value\n",
    "    return class_label\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = pd.read_csv('西瓜数据集2.0α.txt')\n",
    "    # 统计每个特征的取值情况作为全局变量, 空值不算做一个取值\n",
    "    column_count = dict([(ds, list(pd.unique(data.dropna(axis=0,subset = [ds])[ds]))) for ds in data.iloc[:, :-1].columns])\n",
    "    data.insert(0, 'weights', 1) #插入每个样本权值\n",
    "    tree = create_tree(data)\n",
    "    print(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f2b791",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
